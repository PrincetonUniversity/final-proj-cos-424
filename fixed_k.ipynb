{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idempotent data loading. For a given chromosome n (a string).\n",
      "    \n",
      "    Returns (train_df, test_df, train_ix, test_ix, train_tissues, tfs)\n",
      "    \n",
      "    The first two are the train and test dataframes, and test_ix are the\n",
      "    values in test_df['assayed'] that are missing and need to be imputed (with the\n",
      "    correct answer being in test_df['filled'] in the corresponding locations.\n",
      "    train_ix are the assayed (known) methylation values from limited microarray\n",
      "    sampling (e.g., test_df['assayed'].iloc[train_ix] can be used for prediction of\n",
      "    test_df['filled'].iloc[test_ix], and the former should be about equal to\n",
      "    test_df['filled'].iloc[train_ix] (two different ways of sampling methylation).\n",
      "    \n",
      "    Imports genetic context and adds those columns to the parameter df, returning\n",
      "    a merged one. tfs is the list of names of new transcription\n",
      "    factors.\n",
      "    \n",
      "    train_tissues is a list of the names of columns with chromosome methylation values.\n",
      "    \n",
      "    Note that loading from scratch may take ~5 min (for merging genetic contexts).\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import *\n",
    "import sklearn\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import multiprocessing\n",
    "import scipy\n",
    "from joblib import Parallel, delayed\n",
    "import threading\n",
    "nproc = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "if 'utils' not in sys.path:\n",
    "    sys.path.append('utils')\n",
    "\n",
    "import data_loader\n",
    "\n",
    "# Warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Idempotent, cached data retrieval script\n",
    "\n",
    "print(data_loader.load_chromosome.__doc__)\n",
    "train_df, test_df, train_ix, test_ix, train_tissues, tfs = \\\n",
    "    data_loader.load_chromosome_cached('1')\n",
    "    \n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nans in mean-imputed 0\n",
      "nans in interpolated 0\n"
     ]
    }
   ],
   "source": [
    "# Perhaps there are obvious sequence trends?\n",
    "def local_impute(data):\n",
    "    #http://stackoverflow.com/questions/9537543/replace-nans-in-numpy-array-with-closest-non-nan-value\n",
    "    mask = np.isnan(data)\n",
    "    data[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), data[~mask])\n",
    "    return data\n",
    "\n",
    "# Do mean imputation on our training data.\n",
    "def mean_impute(data):\n",
    "    mask = np.isnan(data)\n",
    "    data[mask] = float(data.mean()) # just = m messes with serialization\n",
    "    return data\n",
    "train_df_imp = train_df\n",
    "train_df_int = train_df\n",
    "for i in train_tissues:\n",
    "    train_df_imp[i] = mean_impute(train_df[i].copy())\n",
    "    train_df_int[i] = local_impute(train_df[i].copy())\n",
    "print('nans in mean-imputed', np.isnan(train_df_imp[train_tissues]).sum().sum())\n",
    "print('nans in interpolated', np.isnan(train_df_int[train_tissues]).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copied pretty much directly from sklearn\n",
    "\n",
    "# Returns a TensorFlow scalar with the size of the i-th dimension for\n",
    "# the parameter tensor x.\n",
    "def tf_get_shape(x, i):\n",
    "    return tf.squeeze(tf.slice(tf.shape(x), [i], [1])) \n",
    "\n",
    "def tf_nrows(x):\n",
    "    return tf_get_shape(x, 0)\n",
    "\n",
    "def tf_ncols(x):\n",
    "    return tf_get_shape(x, 1)\n",
    "\n",
    "# Simultaneous K-cluster likelihood computation.\n",
    "# X is NxD, mus is KxD, sigmas is KxD\n",
    "# Output is KxN likelihoods for each sample in each cluster.\n",
    "def tf_log_normals(X, mus, sigmas):\n",
    "    # p(X) = sqrt(a * b * c)\n",
    "    # a = (2 pi)^(-p)\n",
    "    # b = det(sigma)^(-1)\n",
    "    # c = exp(-(x - mu)^T sigma^(-1) (x - mu)) [expanded for numerical stability]\n",
    "    #\n",
    "    # Below we make simplifications since sigma is diag\n",
    "    \n",
    "    D = tf_ncols(mus)\n",
    "    XT = tf.transpose(X) # pxN\n",
    "    invsig = tf.inv(sigmas)\n",
    "    \n",
    "    loga = -tf.cast(D, 'float64') * tf.log(tf.constant(2 * np.pi, dtype='float64')) # scalar\n",
    "    logb = tf.reduce_sum(tf.log(invsig), 1, keep_dims=True) # Kx1\n",
    "    logc =  \\\n",
    "        - tf.reduce_sum(invsig * tf.square(mus), 1, keep_dims=True) \\\n",
    "        + 2 * tf.matmul(invsig * mus, XT) \\\n",
    "        - tf.matmul(invsig, tf.square(XT)) # KxN\n",
    "    \n",
    "    return 0.5 * (loga + logb + logc)\n",
    "\n",
    "# Stably log-sum-exps likelihood along rows.\n",
    "# Reduces KxN tensor L to 1xN tensor\n",
    "def tf_log_sum_exp(L):\n",
    "    maxs = tf.reduce_max(L, 0, keep_dims=True) # 1xN\n",
    "    return tf.log(tf.reduce_sum(tf.exp(L - maxs), 0, keep_dims=True)) + maxs\n",
    "\n",
    "# X is NxD, mus is KxD, sigmas KxD, alphas is K\n",
    "# output is KxN log likelihoods.\n",
    "def tf_log_likelihood(X, mus, sigmas, alphas):\n",
    "    alphas = tf.expand_dims(alphas, 1) # Kx1\n",
    "    return tf_log_normals(X, mus, sigmas) + tf.log(alphas) # KxN\n",
    "\n",
    "# X is NxD, mus is KxD, sigmas KxD, alphas is K\n",
    "# output is 1xN log probability for each sample, KxN responsibilities\n",
    "def estep(X, mus, sigmas, alphas):\n",
    "    log_likelihoods = tf_log_likelihood(X, mus, sigmas, alphas)\n",
    "    sample_log_prob = tf_log_sum_exp(log_likelihoods) # 1xN\n",
    "    return sample_log_prob, tf.exp(log_likelihoods - sample_log_prob)\n",
    "\n",
    "EPS = np.finfo(float).eps\n",
    "MIN_COVAR = 0.1\n",
    "# X is NxD, resp is KxN (and normalized along axis 0)\n",
    "# Returns maximize step means, covariance, and cluster priors,\n",
    "# which have dimension KxD, KxD, and K, respectively\n",
    "def mstep(X, resp):\n",
    "    weights = tf.reduce_sum(resp, 1) # K\n",
    "    invweights = tf.expand_dims(tf.inv(weights + 10 * EPS), 1) # Kx1\n",
    "    alphas = EPS + weights / (tf.reduce_sum(weights) + 10 * EPS) # K\n",
    "    weighted_cluster_sum = tf.matmul(resp, X) # KxD \n",
    "    mus = weighted_cluster_sum * invweights\n",
    "    avg_X2 = tf.matmul(resp, tf.square(X)) * invweights\n",
    "    avg_mu2 = tf.square(mus)\n",
    "    avg_X_mu = mus * weighted_cluster_sum * invweights\n",
    "    sigmas = avg_X2 - 2 * avg_X_mu + avg_mu2 + MIN_COVAR\n",
    "    # (x - mu) (x-mu)^T for banded. \n",
    "    return mus, sigmas, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual likelihoods [[ 0.26088298  0.29452575  0.0762016   0.30748466  0.45977218]\n",
      " [ 0.39989859  0.22567318  0.37302021  0.3812018   0.1812242 ]\n",
      " [ 0.33921843  0.47980106  0.5507782   0.31131355  0.35900362]]\n",
      "log likelihoods    [[ 0.26088298  0.29452575  0.0762016   0.30748466  0.45977218]\n",
      " [ 0.39989859  0.22567318  0.37302021  0.3812018   0.1812242 ]\n",
      " [ 0.33921843  0.47980106  0.5507782   0.31131355  0.35900362]]\n",
      "K=0 rmse=3.2676223177844214e-16\n",
      "K=1 rmse=2.9634845277824204e-16\n",
      "K=2 rmse=3.466670283279959e-16\n"
     ]
    }
   ],
   "source": [
    "# Make sure N, D are small so the numerically unstable verification code\n",
    "# doesn't underflow.\n",
    "N = 5\n",
    "D = 10\n",
    "\n",
    "X = np.random.normal(size=(N, D))\n",
    "mu = X.mean(axis=0)\n",
    "sigma = X.std(axis=0)\n",
    "mus = np.array([mu, mu, mu * 2])\n",
    "sigmas = np.array([sigma, sigma * 2, sigma])\n",
    "K = len(sigmas)\n",
    "alphas = np.random.dirichlet(np.ones(K), 1)[0]\n",
    "\n",
    "mean_ll, resp = sess.run(estep(*(tf.constant(x) for x in (X, mus, sigmas, alphas))))\n",
    "def normal_likelihoods(X, mu, sigma):\n",
    "    exponent = -np.dot((X - mu[np.newaxis, :]) ** 2, 1 / sigma) / 2\n",
    "    return (2 * np.pi) ** (-D / 2) * np.prod(sigma) ** (-1 / 2) * np.exp(exponent)\n",
    "actual = np.array([normal_likelihoods(X, mu, sigma) for mu, sigma in zip(mus, sigmas)])\n",
    "actual = sklearn.preprocessing.normalize(actual * alphas[:, np.newaxis], norm='l1', axis=0)\n",
    "resp = sklearn.preprocessing.normalize(resp, norm='l1', axis=0)\n",
    "print('actual likelihoods', actual)\n",
    "print('log likelihoods   ', resp)\n",
    "rmses = np.sqrt(sklearn.metrics.mean_squared_error(actual.T, resp.T, multioutput='raw_values'))\n",
    "for i, rmse in enumerate(rmses):\n",
    "    print('K={} rmse={}'.format(i, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similar pattern to\n",
    "# https://gist.github.com/narphorium/d06b7ed234287e319f18\n",
    "\n",
    "#todo try initializing covar to emprical cv computed from kmeans labels\n",
    "\n",
    "# Runs up to max_steps EM iterations, stopping earlier if log likelihood improves\n",
    "# less than tol.\n",
    "# X should be an NxD data matrix, initial_mus should be KxD\n",
    "# max_steps should be an int, tol should be a float.\n",
    "def fit_em(X, initial_mus, max_steps, tol, sess):\n",
    "    N, D = X.shape\n",
    "    K, Dmu = initial_mus.shape\n",
    "    assert D == Dmu\n",
    "        \n",
    "    mus0 = initial_mus\n",
    "    sigmas0 = np.tile(np.var(X, axis=0), (K, 1))\n",
    "    alphas0 = np.ones(K) / K\n",
    "    X = tf.constant(X)\n",
    "    \n",
    "    mus, sigmas, alphas = (tf.Variable(x, dtype='float64') for x in [mus0, sigmas0, alphas0])\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    all_ll, resp = estep(X, mus, sigmas, alphas)\n",
    "    cmus, csigmas, calphas = mstep(X, resp)\n",
    "    update_mus_step = tf.assign(mus, cmus)\n",
    "    update_sigmas_step = tf.assign(sigmas, csigmas)\n",
    "    update_alphas_step = tf.assign(alphas, calphas)     \n",
    "    \n",
    "    init_op = tf.initialize_all_variables()\n",
    "    ll = prev_ll = -np.inf\n",
    "                         \n",
    "    with tf.Session() as sess2:\n",
    "        sess2.run(init_op)\n",
    "        for i in range(max_steps):\n",
    "            ll = sess2.run(tf.reduce_mean(all_ll))\n",
    "            sess2.run((update_mus_step, update_sigmas_step, update_alphas_step))\n",
    "            print('EM iteration', i, 'log likelihood', ll)\n",
    "            if abs(ll - prev_ll) < tol:\n",
    "                break\n",
    "            prev_ll = ll\n",
    "        m, s, a = sess2.run((mus, sigmas, alphas))\n",
    "    \n",
    "    return ll, m, s, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a set of partial observations xs each of dimension O < D for a fitted GMM model with \n",
    "# K cluster priors alpha, KxD means mus, and KxD diagonal covariances sigmas,\n",
    "# returns the weighted sum of normals for the remaining D - O dimensions.\n",
    "#\n",
    "# Returns posterior_mus, posterior_sigmas, posterior_prior,\n",
    "# of dimensions:\n",
    "# Kx(D-O), Kx(D-O), NxK, respectively (each mu, sigma is the same for all posteriors).\n",
    "# NxK, NxKxD, NxKxD, respectively, for each x in xs, total of N.\n",
    "def marginal_posterior(xs, mus, sigmas, alphas, sess):\n",
    "    # https://gbhqed.wordpress.com/2010/02/21/conditional-and-marginal-distributions-of-a-multivariate-gaussian/\n",
    "    # diagonal case is easy:\n",
    "    # https://en.wikipedia.org/wiki/Schur_complement#Applications_to_probability_theory_and_statistics\n",
    "    O = xs.shape[1]\n",
    "    D = mus.shape[1]\n",
    "    observed_mus, observed_sigmas = (tf.constant(a, dtype='float64')\n",
    "                                     for a in (mus[:,0:O], sigmas[:, 0:O]))\n",
    "    ll = tf_log_likelihood(xs, observed_mus, observed_sigmas, alphas) # KxN\n",
    "    norm = tf_log_sum_exp(ll) # 1xN\n",
    "    ll, norm = sess.run((ll, norm))\n",
    "    return mus[:, O:D], sigmas[:, O:D], np.transpose(ll - norm)\n",
    "\n",
    "# A \"sparser\" estimate which just uses the most likely cluster's mean as the estimate.\n",
    "def argmax_exp(mus, sigmas, alphas):\n",
    "    i = np.argmax(alphas)\n",
    "    return mus[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit_em() missing 1 required positional argument: 'sess'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0551c63bcaf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_em\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m20.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m20.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit_em() missing 1 required positional argument: 'sess'"
     ]
    }
   ],
   "source": [
    "n_samples = 25\n",
    "np.random.seed(0)\n",
    "\n",
    "def MVN(shear, shift):\n",
    "    rs = np.random.randn(n_samples, 2)\n",
    "    return np.dot(rs, shear.T) + shift\n",
    "G1 = MVN(np.identity(2), np.array([20, 20]))\n",
    "G2 = MVN(np.array([[0., 3.5], [-0.7, .7]]), np.zeros(2))\n",
    "G3 = MVN(np.array([[-1, 0.8], [0, 4]]), np.array([10, 8]))\n",
    "\n",
    "# concatenate the two datasets into the final training set\n",
    "X = np.vstack([G1, G2, G3])\n",
    "rx = np.random.choice(range(len(X)), 3, replace=False)\n",
    "\n",
    "# http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html\n",
    "_, m, s, a = fit_em(X, X[rx], 100, EPS)\n",
    "x = np.linspace(-20.0, 30.0)\n",
    "y = np.linspace(-20.0, 40.0)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "pts = np.array([xx.ravel(), yy.ravel()]).T\n",
    "ll = -estep(pts, m, s, a)[0].eval()\n",
    "print('means\\n{}\\ncov\\n{}\\ncluster priors\\n{}'.format(m, s, a))\n",
    "CS = plt.contour(xx, yy, ll.reshape(xx.shape), norm=LogNorm(vmin=1.0, vmax=1000.0),\n",
    "                levels=np.logspace(0, 3, 20))\n",
    "plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-150a937ab6b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Reverse m, s, a, since we know 'y'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarginal_posterior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "# Make this into a nice image.\n",
    "ys = [-10, 0, 2.5, 10, 20]\n",
    "pts = np.arange(-20, 30, 0.1, dtype='float64').reshape(-1, 1)\n",
    "ys = np.array(ys, dtype='float64').reshape(-1, 1)\n",
    "# Reverse m, s, a, since we know 'y'\n",
    "mr, sr = (x[:, ::-1] for x in (m, s))\n",
    "mm, sm, ams = marginal_posterior(ys, mr, sr, a)\n",
    "for i, y in enumerate(ys.reshape(-1)):\n",
    "    print('y =', y)\n",
    "    ll = estep(pts, mm, sm, ams[i])[0].eval().reshape(-1)\n",
    "    plt.plot(pts, np.exp(ll) / 10 + y)\n",
    "    #plt.show()\n",
    "\n",
    "ys = np.arange(-20, 30, 0.05, dtype='float64').reshape(-1, 1)\n",
    "mm, sm, ams = marginal_posterior(ys, mr, sr, a)\n",
    "xs = [argmax_exp(mm, sm, am) for am in ams]\n",
    "plt.plot(xs, ys, label='argmax cluster mu')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 379551)\n"
     ]
    }
   ],
   "source": [
    "# N = 10\n",
    "N = 33\n",
    "p = 20 # D is better?\n",
    "K = 6\n",
    "k = 0 # maximum band offset (0 is just diagonal)\n",
    "tissues_to_cluster = train_tissues\n",
    "X_np = train_df_imp[train_tissues].values.transpose()\n",
    "\n",
    "np.random.seed(2)\n",
    "rc = np.random.choice(range(len(train_df)), D, replace=False)\n",
    "rmu = np.random.choice(range(N), K, replace=False)\n",
    "\n",
    "np.random.shuffle(X_np)\n",
    "per = N // K\n",
    "splits = [X_np[i:i+per] for i in range(0, N // K * K, per)]\n",
    "\n",
    "\n",
    "# X_trunc = X_np[:N, rc]\n",
    "X_trunc = X_np[:N]\n",
    "mu_init = X_trunc[rmu]\n",
    "# mu_init = np.array([x.mean(axis=0) for x in splits])\n",
    "#mu_init = X_np[19:22]\n",
    "\n",
    "\n",
    "print(mu_init.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 379551)\n"
     ]
    }
   ],
   "source": [
    "print(X_np.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_np.shape (34, 379551)\n",
      "(30, 379551)\n",
      "[  1084   1154   1214 ..., 378792 378806 379168]\n",
      "K 5\n",
      "NUM_RESTARTS 1\n",
      "X_perm.shape (30, 379551)\n",
      "EM iteration 0 log likelihood 104834.753505\n",
      "EM iteration 1 log likelihood 31736.3861947\n",
      "EM iteration 2 log likelihood 51102.112022\n",
      "EM iteration 3 log likelihood 52912.2714273\n",
      "EM iteration 4 log likelihood 53244.2041499\n",
      "EM iteration 5 log likelihood 53719.200865\n",
      "EM iteration 6 log likelihood 53719.200865\n",
      "Done picking best EM\n",
      "368411 368411\n",
      "rmse 0.0698955993458\n",
      "r2 0.829151582451\n",
      "[[-1691.52418439     0.         -1930.65356754 -1114.06471985\n",
      "  -4064.88742012]]\n"
     ]
    }
   ],
   "source": [
    "# Fix a value of K. Perform NUM_RESTARTS random restarts, and pick EM fit with highest mean likelihood \n",
    "# X_np_new = data matrix with bad samples deleted.\n",
    "# X_perm   = permute(X_np_new)\n",
    "# K        = number of clusters\n",
    "# NUM_RESTARTS = number of random restarts\n",
    "\n",
    "K = 5\n",
    "NUM_RESTARTS = 1\n",
    "\n",
    "def get_permutation():\n",
    "    o = np.ones(len(train_df))\n",
    "    o[train_ix] = 0\n",
    "    o[test_ix] = 0\n",
    "    unobserved_untested_ix = np.where(o)[0]\n",
    "    o = np.zeros(len(train_df))\n",
    "    o[test_ix] = 1\n",
    "    unobserved_tested_ix = np.where(o)[0]\n",
    "    permutation = np.hstack((train_ix, unobserved_tested_ix, unobserved_untested_ix))\n",
    "    return permutation, unobserved_tested_ix, unobserved_untested_ix\n",
    "\n",
    "def fit_model(X_train, unobserved_tested_ix, unobserved_untested_ix):\n",
    "    tf.reset_default_graph()\n",
    "    lp = None\n",
    "    m = None\n",
    "    s = None\n",
    "    a = None\n",
    "    for j in range(NUM_RESTARTS):\n",
    "        rmu = np.random.choice(range(X_train.shape[0]), K, replace=False)\n",
    "        mu_init = X_train[rmu]\n",
    "        cur_lp, cur_m, cur_s, cur_a = fit_em(X_train, mu_init, 100, EPS, tf.Session())\n",
    "        if lp is None or cur_lp > lp:\n",
    "            lp = cur_lp\n",
    "            m = cur_m\n",
    "            s = cur_s\n",
    "            a = cur_a\n",
    "    print(\"Done picking best EM\")\n",
    "    \n",
    "    # X is NxD, mus is KxD, sigmas KxD, alphas is K\n",
    "    # output is 1xN log probability for each sample, KxN responsibilities\n",
    "\n",
    "    observed = test_df['filled'][train_ix].values\n",
    "    marginal_means, marginal_covs, marginal_alphas = marginal_posterior(observed.reshape(1, len(observed)), m, s, a, tf.Session())\n",
    "\n",
    "    pred = argmax_exp(marginal_means, marginal_covs, marginal_alphas[0])[:len(unobserved_tested_ix)]\n",
    "    actual = test_df['filled'][unobserved_tested_ix]\n",
    "    print(len(pred), len(actual))\n",
    "    rmse = sklearn.metrics.mean_squared_error(actual, pred)\n",
    "    print('rmse', np.sqrt(rmse)) # rmse of GMM\n",
    "    r2 = sklearn.metrics.r2_score(actual, pred)\n",
    "    print('r2', r2)\n",
    "    return marginal_means, marginal_covs, marginal_alphas, lp, m, s, a\n",
    "    \n",
    "    \n",
    "'''\n",
    "    observed = X_test[:len(train_ix)]\n",
    "    marginal_means, marginal_covs, marginal_alphas = marginal_posterior(observed.reshape(1, len(observed)), m, s, a, sess)\n",
    "\n",
    "    pred = argmax_exp(marginal_means, marginal_covs, marginal_alphas[0])[:len(unobserved_tested_ix)]\n",
    "    actual = X_test[len(train_ix):len(train_ix)+len(unobserved_tested_ix)]\n",
    "    print(len(pred), len(actual))\n",
    "    \n",
    "    # Compute the rmse and r2\n",
    "    mse = sklearn.metrics.mean_squared_error(actual, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print('rmse', rmse) # rmse of GMM\n",
    "    r2 = sklearn.metrics.r2_score(actual, pred)\n",
    "    print('r2', r2)\n",
    "'''\n",
    "\n",
    "print(\"X_np.shape\", X_np.shape) \n",
    "\n",
    "# delete bad samples\n",
    "X_np_new = X_np[:][:33]\n",
    "X_np_new = np.delete(X_np_new, [14,25,26], 0)\n",
    "print(X_np_new.shape)\n",
    "\n",
    "perm, unobserved_tested_ix, unobserved_untested_ix = get_permutation()\n",
    "print(perm)\n",
    "X_perm = X_np_new[:, perm]\n",
    "\n",
    "print(\"K\", K)\n",
    "print(\"NUM_RESTARTS\", NUM_RESTARTS)\n",
    "print(\"X_perm.shape\", X_perm.shape) \n",
    "\n",
    "#KxN responsibilities\n",
    "marginal_means, marginal_covs, marginal_alphas, marginal_logp, mus, sigs, alphas = fit_model(X_perm, unobserved_tested_ix, unobserved_untested_ix)\n",
    "print(marginal_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "(5, 372028)\n",
      "(5, 372028)\n",
      "before: -8801.12989189\n",
      "after: 1.0\n",
      "[14 39 41  4  2]\n",
      "[ 0.91995043  0.93998369  0.92546759 ...,  0.19855093  0.76246235\n",
      "  0.91682623]\n"
     ]
    }
   ],
   "source": [
    "print(marginal_alphas.shape)\n",
    "print(marginal_covs.shape)\n",
    "print(marginal_means.shape)\n",
    "\n",
    "K = marginal_alphas.shape[1]\n",
    "\n",
    "print(\"before:\", np.sum(marginal_alphas))\n",
    "alphs = np.exp(marginal_alphas)\n",
    "print(\"after:\", np.sum(alphs))\n",
    "alphs = np.ndarray.tolist(alphs[0,:])\n",
    "\n",
    "SAMPLE_SIZE = 100\n",
    "\n",
    "# Pick a random clusters\n",
    "samples = np.random.choice(len(alphas), SAMPLE_SIZE, p=alphas)\n",
    "cnts = np.bincount(samples)\n",
    "print(cnts)\n",
    "\n",
    "print(marginal_means[1,:])\n",
    "\n",
    "#S = [] #np.zeros((SAMPLE_SIZE, 2))\n",
    "#for s in samples:\n",
    "    # Pick a sample from this normal\n",
    "    #np.random.normal(marginal_means[s,:], marginal_covs[s], size=(K,N))\n",
    "    \n",
    "    # Add the sample to the set of samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
