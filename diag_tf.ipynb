{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idempotent data loading. For a given chromosome n (a string).\n",
      "    \n",
      "    Returns (train_df, test_df, train_ix, test_ix, train_tissues, tfs)\n",
      "    \n",
      "    The first two are the train and test dataframes, and test_ix are the\n",
      "    values in test_df['assayed'] that are missing and need to be imputed (with the\n",
      "    correct answer being in test_df['filled'] in the corresponding locations.\n",
      "    train_ix are the assayed (known) methylation values from limited microarray\n",
      "    sampling (e.g., test_df['assayed'].iloc[train_ix] can be used for prediction of\n",
      "    test_df['filled'].iloc[test_ix], and the former should be about equal to\n",
      "    test_df['filled'].iloc[train_ix] (two different ways of sampling methylation).\n",
      "    \n",
      "    Imports genetic context and adds those columns to the parameter df, returning\n",
      "    a merged one. tfs is the list of names of new transcription\n",
      "    factors.\n",
      "    \n",
      "    train_tissues is a list of the names of columns with chromosome methylation values.\n",
      "    \n",
      "    Note that loading from scratch may take ~5 min (for merging genetic contexts).\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import *\n",
    "import sklearn\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import multiprocessing\n",
    "nproc = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "if 'utils' not in sys.path:\n",
    "    sys.path.append('utils')\n",
    "\n",
    "import data_loader\n",
    "\n",
    "# Warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Idempotent, cached data retrieval script\n",
    "\n",
    "print(data_loader.load_chromosome.__doc__)\n",
    "train_df, test_df, train_ix, test_ix, train_tissues, tfs = \\\n",
    "    data_loader.load_chromosome_cached('1')\n",
    "    \n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nans in mean-imputed 0\n",
      "nans in interpolated 0\n"
     ]
    }
   ],
   "source": [
    "# Perhaps there are obvious sequence trends?\n",
    "def local_impute(data):\n",
    "    #http://stackoverflow.com/questions/9537543/replace-nans-in-numpy-array-with-closest-non-nan-value\n",
    "    mask = np.isnan(data)\n",
    "    data[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), data[~mask])\n",
    "    return data\n",
    "\n",
    "# Do mean imputation on our training data.\n",
    "def mean_impute(data):\n",
    "    mask = np.isnan(data)\n",
    "    data[mask] = float(data.mean()) # just = m messes with serialization\n",
    "    return data\n",
    "train_df_imp = train_df\n",
    "train_df_int = train_df\n",
    "for i in train_tissues:\n",
    "    train_df_imp[i] = mean_impute(train_df[i].copy())\n",
    "    train_df_int[i] = local_impute(train_df[i].copy())\n",
    "print('nans in mean-imputed', np.isnan(train_df_imp[train_tissues]).sum().sum())\n",
    "print('nans in interpolated', np.isnan(train_df_int[train_tissues]).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copied pretty much directly from sklearn\n",
    "\n",
    "# Returns a TensorFlow scalar with the size of the i-th dimension for\n",
    "# the parameter tensor x.\n",
    "def tf_get_shape(x, i):\n",
    "    return tf.squeeze(tf.slice(tf.shape(x), [i], [1])) \n",
    "\n",
    "def tf_nrows(x):\n",
    "    return tf_get_shape(x, 0)\n",
    "\n",
    "def tf_ncols(x):\n",
    "    return tf_get_shape(x, 1)\n",
    "\n",
    "# Simultaneous K-cluster likelihood computation.\n",
    "# X is NxD, mus is KxD, sigmas is KxD\n",
    "# Output is KxN likelihoods for each sample in each cluster.\n",
    "def tf_log_normals(X, mus, sigmas):\n",
    "    # p(X) = sqrt(a * b * c)\n",
    "    # a = (2 pi)^(-p)\n",
    "    # b = det(sigma)^(-1)\n",
    "    # c = exp(-(x - mu)^T sigma^(-1) (x - mu)) [expanded for numerical stability]\n",
    "    #\n",
    "    # Below we make simplifications since sigma is diag\n",
    "    \n",
    "    D = tf_ncols(mus)\n",
    "    XT = tf.transpose(X) # pxN\n",
    "    invsig = tf.inv(sigmas)\n",
    "    \n",
    "    loga = -tf.cast(D, 'float64') * tf.log(tf.constant(2 * np.pi, dtype='float64')) # scalar\n",
    "    logb = tf.reduce_sum(tf.log(invsig), 1, keep_dims=True) # Kx1\n",
    "    logc =  \\\n",
    "        - tf.reduce_sum(invsig * tf.square(mus), 1, keep_dims=True) \\\n",
    "        + 2 * tf.matmul(invsig * mus, XT) \\\n",
    "        - tf.matmul(invsig, tf.square(XT)) # KxN\n",
    "    return 0.5 * (loga + logb + logc)\n",
    "\n",
    "# Stably log-sum-exps likelihood along rows.\n",
    "# Reduces KxN tensor L to 1xN tensor\n",
    "def tf_log_sum_exp(L):\n",
    "    maxs = tf.reduce_max(L, 0, keep_dims=True) # 1xN\n",
    "    return tf.log(tf.reduce_sum(tf.exp(L - maxs), 0, keep_dims=True)) + maxs\n",
    "\n",
    "# X is NxD, mus is KxD, sigmas KxD, alphas is K\n",
    "# output is log probability, KxN responsibilities\n",
    "def estep(X, mus, sigmas, alphas):\n",
    "    alphas = tf.expand_dims(alphas, 1) # Kx1\n",
    "    log_likelihood = tf_log_normals(X, mus, sigmas) + tf.log(alphas) # KxN\n",
    "    sample_log_prob = tf_log_sum_exp(log_likelihood) # 1xN\n",
    "    return tf.reduce_mean(sample_log_prob), tf.exp(log_likelihood - sample_log_prob)\n",
    "\n",
    "EPS = np.finfo(float).eps\n",
    "MIN_COVAR = EPS\n",
    "# X is NxD, resp is KxN (and normalized along axis 0)\n",
    "# Returns maximize step means, covariance, and cluster priors,\n",
    "# which have dimension KxD, KxD, and K, respectively\n",
    "def mstep(X, resp):\n",
    "    weights = tf.reduce_sum(resp, 1) # K\n",
    "    invweights = tf.expand_dims(tf.inv(weights + 10 * EPS), 1) # Kx1\n",
    "    alphas = EPS + weights / (tf.reduce_sum(weights) + 10 * EPS) # K\n",
    "    weighted_cluster_sum = tf.matmul(resp, X) # KxD \n",
    "    mus = weighted_cluster_sum * invweights\n",
    "    avg_X2 = tf.matmul(resp, tf.square(X)) * invweights\n",
    "    avg_mu2 = tf.square(mus)\n",
    "    avg_X_mu = mus * weighted_cluster_sum * invweights\n",
    "    sigmas = avg_X2 - 2 * avg_X_mu + avg_mu2 + MIN_COVAR\n",
    "    # (x - mu) (x-mu)^T for banded. \n",
    "    return mus, sigmas, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual likelihoods [[ 0.56004977  0.55904159  0.77913312  0.67623742  0.35557478]\n",
      " [ 0.1019229   0.29423096  0.13132855  0.28109013  0.44749144]\n",
      " [ 0.33802733  0.14672746  0.08953833  0.04267245  0.19693378]]\n",
      "log likelihoods    [[ 0.56004977  0.55904159  0.77913312  0.67623742  0.35557478]\n",
      " [ 0.1019229   0.29423096  0.13132855  0.28109013  0.44749144]\n",
      " [ 0.33802733  0.14672746  0.08953833  0.04267245  0.19693378]]\n",
      "K=0 rmse=1.6653345369377348e-16\n",
      "K=1 rmse=1.1188630228279524e-16\n",
      "K=2 rmse=1.1325499295668598e-16\n"
     ]
    }
   ],
   "source": [
    "# Make sure N, D are small so the numerically unstable verification code\n",
    "# doesn't underflow.\n",
    "N = 5\n",
    "D = 10\n",
    "\n",
    "X = np.random.normal(size=(N, D))\n",
    "mu = X.mean(axis=0)\n",
    "sigma = X.std(axis=0)\n",
    "mus = np.array([mu, mu, mu * 2])\n",
    "sigmas = np.array([sigma, sigma * 2, sigma])\n",
    "K = len(sigmas)\n",
    "alphas = np.random.dirichlet(np.ones(K), 1)[0]\n",
    "\n",
    "mean_ll, resp = sess.run(estep(*(tf.constant(x) for x in (X, mus, sigmas, alphas))))\n",
    "def normal_likelihoods(X, mu, sigma):\n",
    "    exponent = -np.dot((X - mu[np.newaxis, :]) ** 2, 1 / sigma) / 2\n",
    "    return (2 * np.pi) ** (-D / 2) * np.prod(sigma) ** (-1 / 2) * np.exp(exponent)\n",
    "actual = np.array([normal_likelihoods(X, mu, sigma) for mu, sigma in zip(mus, sigmas)])\n",
    "actual = sklearn.preprocessing.normalize(actual * alphas[:, np.newaxis], norm='l1', axis=0)\n",
    "resp = sklearn.preprocessing.normalize(resp, norm='l1', axis=0)\n",
    "print('actual likelihoods', actual)\n",
    "print('log likelihoods   ', resp)\n",
    "rmses = np.sqrt(sklearn.metrics.mean_squared_error(actual.T, resp.T, multioutput='raw_values'))\n",
    "for i, rmse in enumerate(rmses):\n",
    "    print('K={} rmse={}'.format(i, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_samples = 10\n",
    "\n",
    "# generate random sample, two components\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate spherical data centered on (20, 20)\n",
    "shifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])\n",
    "\n",
    "# generate zero centered stretched Gaussian data\n",
    "C = np.array([[0., -0.7], [3.5, .7]])\n",
    "stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)\n",
    "\n",
    "# concatenate the two datasets into the final training set\n",
    "X = np.vstack([shifted_gaussian, stretched_gaussian])\n",
    "\n",
    "rx = np.random.choice(range(len(X)), 2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM iteration 0 log likelihood -6.57183821253\n",
      "EM iteration 1 log likelihood -5.2962980488\n",
      "EM iteration 2 log likelihood -3.99620712358\n",
      "EM iteration 3 log likelihood -3.99620712358\n",
      "EM iteration 4 log likelihood -3.99620712358\n",
      "Converged in 4 iterations\n",
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGFhJREFUeJzt3XuQVOWd//H3d7oHmGG4OMgAzjBcIhDEsIBKNN7auKgx\nqxhry5hYtdlc9pfdTZkUqWw0W7sB/SWlJrXsekksf9FNadRkjSWKFxSN2xCuIjByZ1Ac5DoIzDAD\nDMylv78/piUjMDA90z09/fB5VXXR55w+z/n2qcOnzzz9nNPm7oiISDjysl2AiIikl4JdRCQwCnYR\nkcAo2EVEAqNgFxEJjIJdRCQwHQ52Myszs7fNbL2ZrTWzO5PzZ5rZDjNblXzckLlyRUTkTKyj49jN\nbCgw1N0rzKwIWAlMB74K1Lv77MyVKSIiHRXt6AvdfQ+wJ/n8kJltBEqTiy0DtYmISCd0qo/dzEYC\nk4DlyVnfM7MKM3vczAakqTYREemElIM92Q3zPPADdz8E/Br4jLtPovWMXl0yIiJZ1OE+dgAziwKv\nAPPc/cFTLB8BvOzuE0+xTDelERHpBHdPqbs71TP2/wY2tA315Jeqn7gVWHea4nL2MXPmzKzXcLbW\nn8u1q/7sP3K9/s7o8JenZnY5cAew1sxWAw78K/B1M5sEJIAq4LudqkRERNIilVExi4HIKRa9nr5y\nRESkq3TlaQfFYrFsl9AluVx/LtcOqj/bcr3+zkjpy9MubcjMu2tbIiKhMDM8w1+eiohID6dgFxEJ\njIJdRCQwCnYRkcAo2EVEAqNgFxEJjIJdRCQwCnYRkcAo2EVEAqNgFxEJjIJdRCQwHb67o4hIaBoa\nGli2dCn1Bw8y5rOfZfz48dkuKS10EzAROSsdO3aMhx54gOb332dAr1581NjI33znO1xx1VXZLu1T\ndBMwEZEO2rhxIw0ffMBlo0dzQVkZlw8bxqvPPdfpXy3qSRTsInJWam5uprf95US4d34+TceOZbGi\n9FGwi8hZ6fzzz6e+qIj39+zhwKFDLN+2jUuuvhqzlHo9eiT1sYvIWWvnzp3Mfe456mpqGD9pEl+6\n6Sby8/OzXdandKaPXcEuIsGqqalh/quvcnD/fsZOnMhVV19NXl5udVR0Jtg13FFEgnT48GEevu8+\nBu7fT3FhIW+/+y51NTXcfOut2S4t43Lro0tEpIMqKyuJfPwxE4cPp2zQIC4vL2fhvHlBjHo5EwW7\niAQp2YVxfDr8OP8LdcWISJDGjRvHK8OGsXrbNs4pLOSDujq+eNttQYx6ORN9eSoiwaqrq+NP8+e3\nfnl64YVc9oUv5Fywa1SMiEhgdEsBERFRH7uIhCWRSPDq3LksefNNIpEIf/2Vr3B1LEZLSwuVlZU0\nNjYycuRIBg4ceNp2mpubeXXuXN5bupSCoiKmf+1rjB07tpveRdd0uCvGzMqAp4AhQAL4jbs/ZGbn\nAP8DjACqgNvc/eAp1ldXjIhk3J/efJOFTz3FpcOH09TSwrLdu/nKnXeyLB5n37p19IlEOFxYyD/e\nfTdlZWXttjPn+edZ8+KLTC4tpb6hgTVHjvCDe+/lvPPO68Z3k/mumGbgh+4+AbgM+J6ZfRa4G3jL\n3ccBbwM/SaUAEZF02rBqFeMHDaKgVy/6FxQwqqCA1195hZo1a4iNHMll5eUMrq3lx//0T/zHvffy\n1vz5JBKJk9qpWLyYi8rK6F9QQGlxMSXNzVRWVmbhHaWuw8Hu7nvcvSL5/BCwESgDpgNPJl/2JHBL\nuosUEemoooEDqT1y5Ph0XfKOjQOjUcyMw4cP89HatTRv3055bS0Lf/c73njttZPa6VNYyOE2d3s8\nmkjQu3fvzL+BNOjUl6dmNhKYBCwDhrh7NbSGP1CSruJERFL1penT2dGnD8urqlhSVUXD0KF8efp0\ndrW0cOjoUaqrq9l76BAXjB6NHz1KZMcOHvvlL1mzZg0ALS0tbNq0iTGTJ7Nk714qqqpY8sEHREeN\nYtKkSVl+dx2T8nBHMysC4sD/dfeXzOyAuxe3Wb7f3QedYj31sYtIt6itrWXz5s3k5eUxYcIECgsL\nWbRwIXOffZad27fTcuAAN//VX7Fh2TLygU2RCCMuvJDbZ8xg+cKF7Fi5ksJIhF1NTUydNo3Rn/kM\nU6ZMoaCgoNvfS8bHsZtZFHgFmOfuDybnbQRi7l5tZkOB/3X3k3440Mx85syZx6djsRixWCyVWkVE\nusTdqaur46H77mPnwoX0rqmhtndvvjB1Kr2iUaoGDMB27ODq0aMxMz7at4+9JSX86Kc/7bYa4/E4\n8Xj8+PQ999yT8WB/Ctjn7j9sM+8B4IC7P2BmdwHnuPvdp1hXZ+wi0iPU1dVx36xZHFyzhsvGj2f4\noEFU7t5NVb9+nLtvH1NGjgSgobGRxYcO8bOHHsparRkdFWNmlwN3AF80s9VmtsrMbgAeAKaZ2Wbg\nWuD+VAoQEelu/fv3559nzKD/6NEcPHKE9du3s9WdG2+5hWp36hsaSLizdtcuxk6cmO1yU6ZbCohI\n8Hbt2sW6tWuJRCJcdPHFxy9O2rlzJ8uXLAF3LrnsMoYPH86ypUt54cknaT52jPEXXcQd3/oWhYWF\nWatd94oRETnBhx9+yGP33cfQxkaa3akfNIgf/Nu/UVxc3O467k5LSwvRaJR169axvqKCvv36cWUs\nxoABA7qxet0rRkTkJPPnzmVcNMqkkSO5eNQoBtbUsHTRotOuY2ZEo1GWLlnC07/4BQf//GcqX3yR\n//rZz6ivr++myjtPwS4iQTt6+DAFvXodny6IRjna0NChdd+cM4epJSWMHTaMKSNG0HvvXtatW5ep\nUtNGwS4iQbvoiitYu38/++vr2VNby4dNTXxu8uQOrdvS3Ex+JHJ8OkLrBUw9ne7uKCJBu/zKK2lq\nbmbZn/5END+fr3/zmx2+S+MXpk1j0e9/z4RBg6hraKCmqIjx40+6TKfH0ZenIiLtSCQSLFywgLXv\nvEPf/v25/uabKS0t7dYaNCpGRCQwGhUjIiIKdhGR0CjYRUQCo2AXEQmMgl1EJDAKdhGRwCjYRUQC\no2AXEQmMgl1EJDAKdhGRwCjYRUQCo2AXEQmMgl1EJDAKdhGRwCjYRUQCo2AXEQmMgl1EJDAKdhGR\nwCjYRUQCo2AXEQmMgl1EJDAKdhGRwHQ42M3sCTOrNrM1bebNNLMdZrYq+bghM2WKiEhHpXLG/lvg\n+lPMn+3uU5KP19NUl4iIdFKHg93dFwE1p1hk6StHRES6Kh197N8zswoze9zMBqShPRER6YJoF9f/\nNXCvu7uZ/QyYDXy7vRfPmjXr+PNYLEYsFuvi5kVEwhKPx4nH411qw9y94y82GwG87O4TU1mWXO6p\nbEtERMDMcPeUurxT7Yox2vSpm9nQNstuBdal2J6IiKRZh7tizOxZIAYMMrOPgJnANWY2CUgAVcB3\nM1CjiIikIKWumC5tSF0xIiIp646uGBER6eEU7CIigVGwi4gERsEuIhIYBbuISGAU7CIigVGwi4gE\nRsEuIhIYBbuISGAU7CIigVGwi4gERsEuIhIYBbuISGAU7CIigVGwi4gERsEuIhIYBbuISGAU7CIi\ngVGwi4gERsEuIhIYBbuISGAU7CIigVGwi4gERsEuIhIYBbuISGAU7CIigVGwi4gERsEuIhKYDge7\nmT1hZtVmtqbNvHPMbL6ZbTazN8xsQGbKFBGRjkrljP23wPUnzLsbeMvdxwFvAz9JV2EiItI5HQ52\nd18E1JwwezrwZPL5k8AtaapLREQ6qat97CXuXg3g7nuAkq6XJCIiXRFNc3t+uoWzZs06/jwWixGL\nxdK8eRGR3BaPx4nH411qw9xPm8WffrHZCOBld5+YnN4IxNy92syGAv/r7uPbWddT2ZaIiICZ4e6W\nyjqpdsVY8vGJucDfJ59/A3gpxfZERCTNOnzGbmbPAjFgEFANzAReBP4IDAe2Abe5e2076+uMXUQk\nRZ05Y0+pK6YrFOwiIqnrjq4YERHp4RTsIiKBUbCLiARGwS4iEhgFu4hIYBTsIiKBUbCLiARGwS4i\nEhgFu4hIYBTsIiKBUbCLiARGwS4iEhgFu4hIYBTsIiKBUbCLiARGwS4iEhgFu4hIYBTsIiKBUbCL\niARGwS4iEhgFu4hIYBTsIiKBUbCLiARGwS4iEhgFu4hIYBTsIiKBUbCLiARGwS4iEphoOhoxsyrg\nIJAAmtx9ajraFRGR1KUl2GkN9Ji716SpPRER6aR0dcVYGtsSEZEuSFcYO/CGma0ws39IU5siItIJ\n6eqKudzdd5vZYOBNM9vo7otOfNGsWbOOP4/FYsRisTRtXkQkDPF4nHg83qU2zN3TU80nDZrNBOrd\nffYJ8z3d2xIRCZ2Z4e6Wyjpd7ooxs0IzK0o+7wtcB6zrarsiItI56eiKGQLMMTNPtveMu89PQ7si\nItIJae+KaXdD6ooREUlZVrpiRESkZ1Gwi4gERsEuIhIYBbuISGAU7CIigVGwi4gERsEuIhIYBbuI\nSGAU7CIigVGwi4gERsEuIhIYBbuISGAU7CIigUnXLyjJWaS+vp4XXniZqqpdjBpVyq233kRRUVG2\nyxKRJN22V1LS3NzM/ff/Fx98kEdxcTkHDmxjzJg87rrr+0QikWyXJxIc3bZXMm7v3r1s3VpDeflk\n+vU7l/LyKWzZ8jH79u3LdmkikqRgl5REo1ESiWbcEwC4J3BvJhpVr55IT6H/jZKSwYMHc8UVF7Bg\nwSIKCobS0LCba675HMXFxdkuTUSS1McuKWtpaWHp0mXs3LmHsrJhXHrp59W/LpIhneljV7CLiPRg\n+vJUREQU7CIioVGwi4gERsEuIhIYBbuISGAU7CIigVGwi4gERleenmXcnRUr3mXZsgoKCnpz443X\nUlpamu2yRCSN0nLGbmY3mNkmM6s0s7vS0aZkxuLFS3jkkTlUVvZmxYoGfv7zX7F3797jy92dPXv2\nsHXrVo4cOZLFSkWks7p8xm5mecAjwLXALmCFmb3k7pu62racWiKRYPXq1ezeXc155w1l8uTJmHXs\nwrTXX/8zgwdPoV+/cwGoqjpMRcV7XHfdNNydP/5xDq+9tpxIpJCiomb+5V++S1lZWSbfjoikWTq6\nYqYCW9x9G4CZ/QGYDijYM8Ddefrp/+HNNzeSnz+Ypqal3HjjB9x++992KNzNWtto294n61VWVvLK\nK+8yYsRfE4nk8/HH2/jNb57hnnsy80dYc3Mzzz//IgsWvEufPr346le/zKWXfv5Tr3F3FixYyPLl\na+jXr5Cbb75eHzQiZ5COYC8FtreZ3kFr2EsG7Nu3j7ffrmDUqOvIy4vQ0tLMnDlz2LTpfQ4dOsaF\nF47h9ttvpaCg4JTr33TTtTzyyPM0NIylsfEI/fvXMGXKZAAOHDhAJFJMJJIPQHFxKTt3rsnYe3n5\n5Xm89toGhg+P0djYwKOPzqG4+BzGjh17/DVvvPEWTz+9gHPPvZCtW+tZu/bX3HvvDAYPHpyxukRy\nnUbF5JjGxkby8nqRl9d6N8WmpmbWrv2AbdsKyM+/mHh8J0888XS760+degk/+tEdXHxxL669dij/\n/u/fZ9CgQQAMHTqURGIfjY0NAFRXv8+YMSMy9l5WrlxPScmF5Of3oW/fc4hGy9i8ecunXvPWW0s4\n77xLGDhwKMOGjaGhYRAbNmzIWE0iIUjHGftOoLzNdFly3klmzZp1/HksFiMWi6Vh82eXkpISSksL\n2bFjLcXF5WzZspo+ffIpL/8ceXkRRo68mJUrX6GpqYn8/PxTtjFx4kQmTpx40vxRo0bxjW9czzPP\nvIZ7lNLS/nznO/8nY+9l4MAitm49SFFR673cm5rqKSrq+6nXRKMRmpqaj0/rRz0kdPF4nHg83qU2\nunzbXjOLAJtp/fJ0N/AO8DV333jC63Tb3jSpra3l2WdfYNu2XRQWRtm0aT9jx96ImXH06CFqahby\n6KP3k5fXuT/IGhoaaGhoYMCAARm9z/pHH33Effc9ytGj55BINDJ6dC9+/OM7P9WNtHz5O/zqVy9Q\nUDCaY8cOce65dfz0pz+kf//+GatLpCfJ2v3YzewG4EFau3aecPf7T/Gabgv2RCLBpk2bOHz4MOXl\n5QwZMqRbtpsNLS0tPPzw/2Plyr1EIgNw38O3v/03XHXVFdkurUP279/Pli1byM/PZ8KECfTp0+ek\n16xfv56KivUUFRVy9dVXMHDgwCxUKpId+qENWkP9scd+y7JlVUQi/cjL28+MGX/HhAkTMr7tbGlu\nbmb16tUcPFjHiBHljBkzJtsliUiaKNiBDRs28MADv2fkyGswy6Ou7mNgHbNn35PxbYuIpFtngj24\nb6EOHz5MJNKP1uumoKiomB076j81XrsncHcWLlzEqlXrGTiwH1/+8jRKSkqyXZaIBCC44Y7l5eXk\n5e2nvn4fiUQL27dXMGnSuB4V6gDz5s3n8cfn8+GH/Vi8+CA///nD1NbWZrssEQlAcME+ZMgQZsz4\nOxKJNezY8SqTJxfxrW/dke2yTjJv3kJKSz9PcXEpZWUXUFtbxMaNG8+8oojIGQTXFQMwYcIE/vM/\n7+1x3S9t5eXl4Z5oMyfR6eGJIiJtBZ0kPTXUAaZP/yK7di2junorH31UQUlJIxdccEG2yxKRAAQ3\nKiZXuDsrV66iomI9AwYUMW3aFzU+W0ROouGOIiKB6UywB90VIyJyNlKwi4gERsEuIhIYBbuISGCC\nHMeeLuvWrWPRohXk50eZNu1qysvLz7ySiEiWaVRMO9577z1mz36Wvn0/S3NzI/AhM2feSWlpabZL\nE5GziEbFpNEbb/yZAQMmUlIyivPOG0dLSxlLl76T7bJERM5Iwd6O5Kfk8emefHsCEZG2FOztuP76\nK6mrW0t19VZ27dpMfv5OLrtsarbLEhE5I/Wxn8b69etZvPhdevXK59prr2T48OHZLklEzjK6pYCI\nSGD05amIiCjYRURCo2AXEQmMgl1EJDAKdhGRwCjYRUQCo2AXEQmMgl1EJDAKdhGRwHQp2M1sppnt\nMLNVyccN6SpMREQ6Jx1n7LPdfUry8Xoa2uuR4vF4tkvoklyuP5drB9Wfbblef2ekI9jPinvZ5vrB\nkcv153LtoPqzLdfr74x0BPv3zKzCzB43swFpaE9ERLrgjMFuZm+a2Zo2j7XJf28Cfg18xt0nAXuA\n2ZkuWERETi9tt+01sxHAy+4+sZ3lumeviEgnpHrb3mhXNmZmQ919T3LyVmBdugoTEZHO6VKwA78w\ns0lAAqgCvtvlikREpEu67ReURESke2T8ylMz+1szW2dmLWY25YRlPzGzLWa20cyuy3QtXZWLF2SZ\n2Q1mtsnMKs3srmzXkyozqzKz98xstZm9k+16zsTMnjCzajNb02beOWY238w2m9kbPXn0WDv158Rx\nb2ZlZva2ma1PDvL4fnJ+Tuz/U9R/Z3J+yvs/42fsZjaO1q6ax4Afufuq5PzxwLPAJUAZ8BYwpif/\nMKqZzQTq3T0nRv+YWR5QCVwL7AJWALe7+6asFpYCM9sKXOTuNdmupSPM7ArgEPDUJwMJzOwBYL+7\n/yL54XqOu9+dzTrb0079OXHcm9lQYKi7V5hZEbASmA58kxzY/6ep/6ukuP8zfsbu7pvdfQsnX8g0\nHfiDuze7exWwBZia6XrSIJe+BJ4KbHH3be7eBPyB1v2eS4wcuqeRuy8CTvwQmg48mXz+JHBLtxaV\ngnbqhxw47t19j7tXJJ8fAjbSetKYE/u/nfpLk4tz5sesS4HtbaZ38pc30ZPl0gVZJ+7jHeTGPm7L\ngTfMbIWZ/UO2i+mkEnevhtb/vEBJluvpjFw67jGzkcAkYBkwJNf2f5v6lydnpbT/0xLsZ7iIKafo\ngqwe53J3vxi4kdaD+4psF5QGPba7sR05ddwnuzGeB36QPPM9cX/36P1/ivpT3v9dHe4IgLtP68Rq\nO4HhbabLkvOyKoX38hvg5UzWkgY7gfI20z1iH6fC3Xcn//3YzObQ2r20KLtVpazazIa4e3WyH3Vv\ntgtKhbt/3GayRx/3ZhalNRR/5+4vJWfnzP4/Vf2d2f/d3RXTtp9oLnC7mfUys1HA+UCPHvWQPCg+\ncdoLsnqIFcD5ZjbCzHoBt9O633OCmRUmz14ws77AdfT8fQ6tx/mJx/rfJ59/A3jpxBV6mE/Vn2PH\n/X8DG9z9wTbzcmn/n1R/Z/Z/d4yKuQV4GDgXqAUq3P1LyWU/Ab4NNNH6Z8f8jBbTRWb2FK39Xscv\nyPqk766nSg6NepDWD/En3P3+LJfUYckP/Dm0/ukcBZ7p6fWb2bNADBgEVAMzgReBP9L6F+o24DZ3\nr81WjafTTv3XkAPHvZldDiwE1tJ6zDjwr7SeMD5HD9//p6n/66S4/3WBkohIYHJmGJmIiHSMgl1E\nJDAKdhGRwCjYRUQCo2AXEQmMgl1EJDAKdhGRwCjYRUQC8/8B4TWJVeU0XEwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3b57f9630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " _, m, s, a = fit_em(X, X[rx], 100, EPS)\n",
    "_, r = estep(X, m, s, a)\n",
    "r = r.eval()\n",
    "r = r.argmax(axis=0)\n",
    "print(r)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=r, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar pattern to\n",
    "# https://gist.github.com/narphorium/d06b7ed234287e319f18\n",
    "\n",
    "#todo try initializing covar to emprical cv computed from kmeans labels\n",
    "\n",
    "# Runs up to max_steps EM iterations, stopping earlier if log likelihood improves\n",
    "# less than tol.\n",
    "# X should be an NxD data matrix, initial_mus should be KxD\n",
    "# max_steps should be an int, tol should be a float.\n",
    "def fit_em(X, initial_mus, max_steps, tol):\n",
    "    N, D = X.shape\n",
    "    K, Dmu = initial_mus.shape\n",
    "    assert D == Dmu\n",
    "        \n",
    "    mus0 = initial_mus\n",
    "    sigmas0 = np.tile(np.var(X, axis=0), (K, 1))\n",
    "    alphas0 = np.ones(K)\n",
    "    X = tf.constant(X)\n",
    "    \n",
    "    mus, sigmas, alphas = (tf.Variable(x, dtype='float64') for x in [mus0, sigmas0, alphas0])\n",
    "        \n",
    "    mean_ll, resp = estep(X, mus, sigmas, alphas)\n",
    "    cmus, csigmas, calphas = mstep(X, resp)\n",
    "    update_mus_step = tf.assign(mus, cmus)\n",
    "    update_sigmas_step = tf.assign(sigmas, csigmas)\n",
    "    update_alphas_step = tf.assign(alphas, calphas)     \n",
    "    \n",
    "    init_op = tf.initialize_all_variables()\n",
    "    ll = prev_ll = -np.inf\n",
    "                         \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        for i in range(max_steps):\n",
    "            ll = sess.run(mean_ll)\n",
    "            sess.run((update_mus_step, update_sigmas_step, update_alphas_step))\n",
    "            print('EM iteration', i, 'log likelihood', ll)\n",
    "            if abs(ll - prev_ll) < tol:\n",
    "                break\n",
    "            prev_ll = ll\n",
    "        m, s, a = sess.run((mus, sigmas, alphas))\n",
    "    \n",
    "    print('Converged in', i, 'iterations')\n",
    "        \n",
    "    return ll, m, s, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## GMM Stuff\n",
    "np.random.seed(0)\n",
    "rc = np.random.choice(range(len(train_df)), D, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scratch work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_l1normalize_rows(x):\n",
    "    s = tf.reduce_sum(x, 1, keep_dims=True)\n",
    "    return x / s\n",
    "\n",
    "def tf_normal(X, mu, sigma, p):\n",
    "    shifted = X - mu\n",
    "    sq = tf.square(shifted)\n",
    "    det = tf.reduce_prod(sigma)\n",
    "    sigma = tf.expand_dims(sigma, 1)\n",
    "    return tf.exp(-tf.matmul(sq, tf.inv(sigma)) / 2) * tf.rsqrt(det) * (2 * np.pi) ** (-p /2)\n",
    "# if it provides any comfort, tf_normal ~= tf.exp(tf_log_normal)\n",
    "\n",
    "def tf_matrix_row(X, n, p): # X is a matrix of rows length p, gets n-th row\n",
    "    return tf.squeeze(tf.slice(X, [n, 0], [1, p]))\n",
    "\n",
    "# X is Nxp, mus is Kxp, sigmas Kxp\n",
    "# output is NxK responsibilities\n",
    "def responsibilities(X, mus, sigmas, alphas, K, p):\n",
    "    all_norms = []\n",
    "    for i in range(K):\n",
    "        mu = tf_matrix_row(mus, i, p)\n",
    "        sigma = tf_matrix_row(sigmas, i, p)\n",
    "        all_norms.append(tf_normal_scaled(X, mu, sigma, p))\n",
    "    unnorm = tf.concat(1, all_norms) # NxK unnormalized responsibility\n",
    "    return tf_l1normalize_rows(unnorm * alphas)\n",
    "\n",
    "# X is Nxp, mus is Kxp, sigmas Kxp\n",
    "# output is log probability, NxK responsibilities\n",
    "def estep(X, mus, sigmas, alphas, K, p):\n",
    "    log_likelihood = []\n",
    "    for i in range(K):\n",
    "        mu = tf_matrix_row(mus, i, p)\n",
    "        sigma = tf_matrix_row(sigmas, i, p)\n",
    "        log_likelihood.append(tf_log_normal(X, mu, sigma, p))\n",
    "    alphas = tf.expand_dims(alphas, 0) # 1xK\n",
    "    log_likelihood = tf.concat(1, log_likelihood) + tf.log(alphas) # NxK likelihoods\n",
    "    sample_log_prob = tf_log_sum_exp(log_likelihood)\n",
    "    return tf.reduce_mean(sample_log_prob), tf.exp(log_likelihood - sample_log_prob)\n",
    "\n",
    "EPS = np.finfo(float).eps\n",
    "MIN_COVAR = 1e-3\n",
    "def mstep(X, mus, sigmas, alphas, resp):\n",
    "    weights = tf.reduce_sum(resp, 0)\n",
    "    invweights = tf.expand_dims(tf.inv(weights + 10 * EPS), 1) # Kx1\n",
    "    new_alphas = EPS + weights / (tf.reduce_sum(weights) + 10 * EPS)\n",
    "    respT = tf.transpose(resp)\n",
    "    weighted_cluster_sum = tf.matmul(respT, X) # Kxp \n",
    "    new_mus = weighted_cluster_sum * invweights\n",
    "    avg_X2 = tf.matmul(respT, tf.square(X)) * invweights\n",
    "    avg_mu2 = tf.square(mus)\n",
    "    avg_X_mu = mus * weighted_cluster_sum * invweights\n",
    "    new_sigma = avg_X2 - 2 * avg_X_mu + avg_mu2 + MIN_COVAR\n",
    "    # (x - mu) (x-mu)^T for banded. \n",
    "    return new_alphas, new_mus, new_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.19773441e-03,   4.19773255e-03],\n",
       "       [  1.88129321e-02,   4.66325728e-05],\n",
       "       [  3.92013006e-02,   3.25969829e-08],\n",
       "       [  4.66326164e-05,   1.88129321e-02]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(tf_log_normals(a, tf.pack([b, a1, b / 2, a2]), tf.pack([c, c, c / 2, c]))).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "p = 20 # D is better?\n",
    "K = 3\n",
    "k = 0 # maximum band offset (0 is just diagonal)\n",
    "tissues_to_cluster = train_tissues\n",
    "X_np = train_df_imp[train_tissues].values.transpose()\n",
    "X_trunc = X_np[:N, rc].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas0 [ 0.33333334  0.33333334  0.33333334]\n",
      "mus0 [[ 0.84375     0.88524592  0.43076923  0.67605633  0.22222222  0.92307693\n",
      "   0.41860464  0.11111111  0.31428573  0.87037039  0.69565219  0.84507042\n",
      "   0.40816328  0.86842108  0.29090908  0.25        0.86956519  0.80392158\n",
      "   0.91780823  0.04938272]\n",
      " [ 0.83333331  0.875       0.42105263  0.73333335  0.17391305  0.95999998\n",
      "   0.27419356  0.06382979  0.09677419  0.47499999  0.69767439  0.69230771\n",
      "   0.53333336  0.77142859  0.35135135  0.16216215  0.83783782  0.8918919\n",
      "   0.8888889   0.1147541 ]\n",
      " [ 0.72727275  0.953125    0.64179105  0.27710843  0.33870968  0.78181821\n",
      "   0.5         0.10526316  0.69230771  0.87755102  0.3888889   0.87323946\n",
      "   0.22033899  0.91666669  0.33333334  0.05660377  0.89393938  0.70175439\n",
      "   0.89795917  0.9054054 ]]\n",
      "sigmas0 [[ 0.02759033  0.00194256  0.01775867  0.03035834  0.01180523  0.01866075\n",
      "   0.03527383  0.0640904   0.06314699  0.02359549  0.03104059  0.00453064\n",
      "   0.02002618  0.03906937  0.01024649  0.00331109  0.02084198  0.01096721\n",
      "   0.00379936  0.14292412]\n",
      " [ 0.02759033  0.00194256  0.01775867  0.03035834  0.01180523  0.01866075\n",
      "   0.03527383  0.0640904   0.06314699  0.02359549  0.03104059  0.00453064\n",
      "   0.02002618  0.03906937  0.01024649  0.00331109  0.02084198  0.01096721\n",
      "   0.00379936  0.14292412]\n",
      " [ 0.02759033  0.00194256  0.01775867  0.03035834  0.01180523  0.01866075\n",
      "   0.03527383  0.0640904   0.06314699  0.02359549  0.03104059  0.00453064\n",
      "   0.02002618  0.03906937  0.01024649  0.00331109  0.02084198  0.01096721\n",
      "   0.00379936  0.14292412]]\n",
      "pis [[  9.99894142e-01   1.05832572e-04   5.61148550e-09]\n",
      " [  1.05832572e-04   9.99894142e-01   2.44758424e-13]\n",
      " [  9.14698362e-01   8.53015482e-02   9.88628344e-08]\n",
      " [  9.89067137e-01   1.09324250e-02   3.87533163e-07]\n",
      " [  5.61207969e-09   2.44784336e-13   1.00000000e+00]\n",
      " [  9.78490770e-01   1.69457849e-06   2.15075389e-02]\n",
      " [  2.99909821e-04   2.02898545e-10   9.99700069e-01]\n",
      " [  4.93033379e-01   2.82362021e-06   5.06963789e-01]\n",
      " [  9.98631477e-01   3.66453052e-04   1.00204523e-03]\n",
      " [  6.27624094e-01   3.72375250e-01   6.67772952e-07]]\n",
      "alphas1 [ 0.6001845   0.14689802  0.25291747]\n",
      "mus1 [[ 0.7375015   0.89849502  0.43162623  0.53868067  0.24562259  0.82504368\n",
      "   0.27875385  0.23787977  0.49909115  0.78632879  0.58118457  0.84612942\n",
      "   0.43604335  0.68368757  0.25795871  0.23246682  0.81416917  0.80806863\n",
      "   0.88918734  0.33689871]\n",
      " [ 0.81705618  0.87838125  0.42652243  0.69481128  0.19939217  0.91627109\n",
      "   0.22492574  0.22449549  0.22204961  0.51435041  0.68816864  0.73046076\n",
      "   0.53202665  0.74890035  0.33001721  0.1790826   0.7364136   0.84626824\n",
      "   0.88067102  0.15760487]\n",
      " [ 0.70613837  0.94759405  0.5993982   0.32280993  0.22709256  0.74940407\n",
      "   0.54625088  0.24407369  0.75384396  0.91216558  0.38996053  0.90921587\n",
      "   0.36835015  0.87691975  0.38311374  0.13536462  0.87148178  0.79025233\n",
      "   0.9362039   0.89710373]]\n",
      "diff alpha 0.0\n",
      "diff mu 2.11596e-06\n",
      "sigma (3, 20) \n",
      " [[ 0.0553524   0.00332995  0.0204402   0.04277299  0.01538361  0.03155763\n",
      "   0.05211444  0.07405557  0.08416092  0.02120609  0.0479403   0.0024677\n",
      "   0.01817991  0.08817507  0.0123311   0.00160596  0.02709044  0.01373781\n",
      "   0.0068288   0.20208509]\n",
      " [ 0.00461478  0.00117625  0.00339006  0.00601347  0.00382136  0.00769628\n",
      "   0.01004694  0.09770349  0.05669369  0.00697467  0.00197826  0.00556491\n",
      "   0.00103281  0.00274207  0.00261188  0.00191397  0.04655124  0.00879647\n",
      "   0.00153895  0.00953631]\n",
      " [ 0.00179358  0.001211    0.00421785  0.00993147  0.02497559  0.01292033\n",
      "   0.00475453  0.09712843  0.00768028  0.00319333  0.0039178   0.00317503\n",
      "   0.05324861  0.00415064  0.0073813   0.01228856  0.00203784  0.01752849\n",
      "   0.00351752  0.00137003]]\n"
     ]
    }
   ],
   "source": [
    "## Implement custom GMM\n",
    "#todo check float32/float64 conversion for speedup\n",
    "#todo scale (standardize) data before running\n",
    "#todo kmeans warmup (just in numpy?)\n",
    "#todo mincovar\n",
    "\n",
    "X = tf.Variable(X_trunc, 'X') #tf.placeholder('float', shape=[N, p])\n",
    "\n",
    "alpha0 = tf.ones([K], dtype='float') / K\n",
    "alphas = tf.Variable(alpha0, 'alphas')\n",
    "\n",
    "mu0 = X_trunc[np.random.choice(N, K, replace=False)]\n",
    "mus = tf.Variable(mu0, 'mus')\n",
    "\n",
    "# initialize the off-band with covariances\n",
    "# currently just diagonal\n",
    "# once random init\n",
    "def tf_repeat(x, n):\n",
    "    return tf.pack([x for i in range(n)]) # tf.tile instead. todo\n",
    "sigma0 = tf.nn.moments(X_trunc, [0])[1]\n",
    "sigmas = tf.Variable(tf_repeat(sigma0, K), 'sigmas')\n",
    "\n",
    "# E-step\n",
    "pis = responsibilities(X, mus, sigmas, alphas, K, p)\n",
    "\n",
    "# M-step\n",
    "#http://cslu.ohsu.edu/~bedricks/courses/cs655/pdf/addl_slides/pr813_lecture06.pdfttp://cslu.ohsu.edu/~bedricks/courses/cs655/pdf/addl_slides/pr813_lecture06.pdf\n",
    "pisT = tf.transpose(pis)\n",
    "membership = tf.reduce_sum(pisT, 1)\n",
    "mu1 = tf.matmul(pisT, X) / tf.expand_dims(membership, 1)\n",
    "alpha1 = membership / tf.reduce_sum(membership)\n",
    "\n",
    "# use \n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print('alphas0', sess.run(alphas))\n",
    "print('mus0', sess.run(mus))\n",
    "print('sigmas0', sess.run(sigmas))\n",
    "print('pis', sess.run(pis))\n",
    "print('alphas1', sess.run(alpha1))\n",
    "print('mus1', sess.run(mu1))\n",
    "a, m, s = sess.run(mstep(X, mus, sigmas, alphas, pis))\n",
    "print('diff alpha', sess.run(tf.reduce_sum(tf.abs(a - alpha1))))\n",
    "print('diff mu', sess.run(tf.reduce_sum(tf.abs(m - mu1))))\n",
    "print('sigma', s.shape, '\\n', s)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
