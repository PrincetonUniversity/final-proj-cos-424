{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idempotent data loading. For a given chromosome n (a string).\n",
      "    \n",
      "    Returns (train_df, test_df, train_ix, test_ix, train_tissues, tfs)\n",
      "    \n",
      "    The first two are the train and test dataframes, and test_ix are the\n",
      "    values in test_df['assayed'] that are missing and need to be imputed (with the\n",
      "    correct answer being in test_df['filled'] in the corresponding locations.\n",
      "    train_ix are the assayed (known) methylation values from limited microarray\n",
      "    sampling (e.g., test_df['assayed'].iloc[train_ix] can be used for prediction of\n",
      "    test_df['filled'].iloc[test_ix], and the former should be about equal to\n",
      "    test_df['filled'].iloc[train_ix] (two different ways of sampling methylation).\n",
      "    \n",
      "    Imports genetic context and adds those columns to the parameter df, returning\n",
      "    a merged one. tfs is the list of names of new transcription\n",
      "    factors.\n",
      "    \n",
      "    train_tissues is a list of the names of columns with chromosome methylation values.\n",
      "    \n",
      "    Note that loading from scratch may take ~5 min (for merging genetic contexts).\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import *\n",
    "import sklearn\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import multiprocessing\n",
    "nproc = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "if 'utils' not in sys.path:\n",
    "    sys.path.append('utils')\n",
    "\n",
    "import data_loader\n",
    "\n",
    "# Warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Idempotent, cached data retrieval script\n",
    "\n",
    "print(data_loader.load_chromosome.__doc__)\n",
    "train_df, test_df, train_ix, test_ix, train_tissues, tfs = \\\n",
    "    data_loader.load_chromosome_cached('1')\n",
    "    \n",
    "if not sess:\n",
    "    sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nans in mean-imputed 0\n",
      "nans in interpolated 0\n"
     ]
    }
   ],
   "source": [
    "# Perhaps there are obvious sequence trends?\n",
    "def local_impute(data):\n",
    "    #http://stackoverflow.com/questions/9537543/replace-nans-in-numpy-array-with-closest-non-nan-value\n",
    "    mask = np.isnan(data)\n",
    "    data[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), data[~mask])\n",
    "    return data\n",
    "\n",
    "# Do mean imputation on our training data.\n",
    "def mean_impute(data):\n",
    "    mask = np.isnan(data)\n",
    "    data[mask] = float(data.mean()) # just = m messes with serialization\n",
    "    return data\n",
    "train_df_imp = train_df\n",
    "train_df_int = train_df\n",
    "for i in train_tissues:\n",
    "    train_df_imp[i] = mean_impute(train_df[i].copy())\n",
    "    train_df_int[i] = local_impute(train_df[i].copy())\n",
    "print('nans in mean-imputed', np.isnan(train_df_imp[train_tissues]).sum().sum())\n",
    "print('nans in interpolated', np.isnan(train_df_int[train_tissues]).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## GMM Stuff\n",
    "np.random.seed(0)\n",
    "rc = np.random.choice(train_ix, p, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copied pretty much directly from sklearn\n",
    "\n",
    "# Simultaneous K-cluster likelihood computation.\n",
    "# X is Nxp, mus is Kxp, sigmas is Kxp\n",
    "# Output is KxN likelihoods for each sample in each cluster.\n",
    "def tf_log_normals(X, mus, sigmas):\n",
    "    # p(X) = sqrt(a * b * c)\n",
    "    # a = (2 pi)^(-p)\n",
    "    # b = det(sigma)^(-1)\n",
    "    # c = exp(-(x - mu)^T sigma^(-1) (x - mu)) [expanded for numerical stability]\n",
    "    #\n",
    "    # Below we make simplifications since sigma is diag\n",
    "    \n",
    "    p = tf.squeeze(tf.slice(tf.shape(mus), tf.pack([tf.rank(mus) - 1]), [1])) \n",
    "    XT = tf.transpose(X) # pxN\n",
    "    invsig = tf.inv(sigmas)\n",
    "    \n",
    "    loga = -tf.cast(p, 'float64') * tf.log(tf.constant(2 * np.pi, dtype='float64')) # scalar\n",
    "    logb = tf.reduce_sum(tf.log(invsig), 1, keep_dims=True) # Kx1\n",
    "    logc =  \\\n",
    "        - tf.reduce_sum(invsig * tf.square(mus), 1, keep_dims=True) \\\n",
    "        + 2 * tf.matmul(invsig * mus, XT) \\\n",
    "        - tf.matmul(invsig, tf.square(XT)) # KxN\n",
    "    return 0.5 * (loga + logb + logc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1 rmse 1.639614891002561e-13\n",
      "K=1 rmse 1.016845989170083e-13\n",
      "K=1 rmse 1.8189894035458566e-13\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "D = 1000\n",
    "\n",
    "X = np.random.normal(size=(N, D))\n",
    "mu = X.mean(axis=0)\n",
    "sigma = X.std(axis=0)\n",
    "mus = np.array([mu, mu, mu * 2])\n",
    "sigmas = np.array([sigma, sigma * 2, sigma])\n",
    "\n",
    "ll = tf_log_normals(*(tf.constant(x) for x in (X, mus, sigmas))).eval()\n",
    "for i, (mu, sigma) in enumerate(zip(mus, sigmas)):\n",
    "    actual = D * np.log(2 * np.pi) + np.log(np.prod(sigma))\n",
    "    actual += np.sum((X - mu) ** 2 / sigma, axis=1)\n",
    "    actual *= -0.5\n",
    "    print('K=1 rmse', math.sqrt(sklearn.metrics.mean_squared_error(actual, ll[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scratch work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_l1normalize_rows(x):\n",
    "    s = tf.reduce_sum(x, 1, keep_dims=True)\n",
    "    return x / s\n",
    "\n",
    "def tf_normal(X, mu, sigma, p):\n",
    "    shifted = X - mu\n",
    "    sq = tf.square(shifted)\n",
    "    det = tf.reduce_prod(sigma)\n",
    "    sigma = tf.expand_dims(sigma, 1)\n",
    "    return tf.exp(-tf.matmul(sq, tf.inv(sigma)) / 2) * tf.rsqrt(det) * (2 * np.pi) ** (-p /2)\n",
    "# if it provides any comfort, tf_normal ~= tf.exp(tf_log_normal)\n",
    "\n",
    "def tf_matrix_row(X, n, p): # X is a matrix of rows length p, gets n-th row\n",
    "    return tf.squeeze(tf.slice(X, [n, 0], [1, p]))\n",
    "\n",
    "# X is Nxp, mus is Kxp, sigmas Kxp\n",
    "# output is NxK responsibilities\n",
    "def responsibilities(X, mus, sigmas, alphas, K, p):\n",
    "    all_norms = []\n",
    "    for i in range(K):\n",
    "        mu = tf_matrix_row(mus, i, p)\n",
    "        sigma = tf_matrix_row(sigmas, i, p)\n",
    "        all_norms.append(tf_normal_scaled(X, mu, sigma, p))\n",
    "    unnorm = tf.concat(1, all_norms) # NxK unnormalized responsibility\n",
    "    return tf_l1normalize_rows(unnorm * alphas)\n",
    "\n",
    "# all implementation basically copied from scipy gmm\n",
    "# Stably log-sum-exps likelihood along rows.\n",
    "# Reduces NxK tensor L to Nx1 tensor\n",
    "def tf_log_sum_exp(L):\n",
    "    maxs = tf.reduce_max(L, 1, keep_dims=True)\n",
    "    return tf.expand_dims(tf.log(tf.reduce_sum(tf.exp(L - maxs), 1)), 1) + maxs\n",
    "\n",
    "# X is Nxp, mus is Kxp, sigmas Kxp\n",
    "# output is log probability, NxK responsibilities\n",
    "def estep(X, mus, sigmas, alphas, K, p):\n",
    "    log_likelihood = []\n",
    "    for i in range(K):\n",
    "        mu = tf_matrix_row(mus, i, p)\n",
    "        sigma = tf_matrix_row(sigmas, i, p)\n",
    "        log_likelihood.append(tf_log_normal(X, mu, sigma, p))\n",
    "    log_likelihood = tf.concat(1, log_likelihood) + tf.log(alphas) # NxK likelihoods\n",
    "    sample_log_prob = tf_log_sum_exp(log_likelihood)\n",
    "    return tf.reduce_mean(sample_log_prob), tf.exp(log_likelihood - sample_log_prob)\n",
    "\n",
    "EPS = np.finfo(float).eps\n",
    "MIN_COVAR = 1e-3\n",
    "def mstep(X, mus, sigmas, alphas, resp):\n",
    "    weights = tf.reduce_sum(resp, 0)\n",
    "    invweights = tf.expand_dims(tf.inv(weights + 10 * EPS), 1) # Kx1\n",
    "    new_alphas = EPS + weights / (tf.reduce_sum(weights) + 10 * EPS)\n",
    "    respT = tf.transpose(resp)\n",
    "    weighted_cluster_sum = tf.matmul(respT, X) # Kxp \n",
    "    new_mus = weighted_cluster_sum * invweights\n",
    "    avg_X2 = tf.matmul(respT, tf.square(X)) * invweights\n",
    "    avg_mu2 = tf.square(mus)\n",
    "    avg_X_mu = mus * weighted_cluster_sum * invweights\n",
    "    new_sigma = avg_X2 - 2 * avg_X_mu + avg_mu2 + MIN_COVAR\n",
    "    # (x - mu) (x-mu)^T for banded. \n",
    "    return new_alphas, new_mus, new_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.19773441e-03,   4.19773255e-03],\n",
       "       [  1.88129321e-02,   4.66325728e-05],\n",
       "       [  3.92013006e-02,   3.25969829e-08],\n",
       "       [  4.66326164e-05,   1.88129321e-02]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(tf_log_normals(a, tf.pack([b, a1, b / 2, a2]), tf.pack([c, c, c / 2, c]))).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "p = 20 # D is better?\n",
    "K = 3\n",
    "k = 0 # maximum band offset (0 is just diagonal)\n",
    "tissues_to_cluster = train_tissues\n",
    "X_np = train_df_imp[train_tissues].values.transpose()\n",
    "X_trunc = X_np[:N, rc].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas0 [ 0.33333334  0.33333334  0.33333334]\n",
      "mus0 [[ 0.84375     0.88524592  0.43076923  0.67605633  0.22222222  0.92307693\n",
      "   0.41860464  0.11111111  0.31428573  0.87037039  0.69565219  0.84507042\n",
      "   0.40816328  0.86842108  0.29090908  0.25        0.86956519  0.80392158\n",
      "   0.91780823  0.04938272]\n",
      " [ 0.83333331  0.875       0.42105263  0.73333335  0.17391305  0.95999998\n",
      "   0.27419356  0.06382979  0.09677419  0.47499999  0.69767439  0.69230771\n",
      "   0.53333336  0.77142859  0.35135135  0.16216215  0.83783782  0.8918919\n",
      "   0.8888889   0.1147541 ]\n",
      " [ 0.72727275  0.953125    0.64179105  0.27710843  0.33870968  0.78181821\n",
      "   0.5         0.10526316  0.69230771  0.87755102  0.3888889   0.87323946\n",
      "   0.22033899  0.91666669  0.33333334  0.05660377  0.89393938  0.70175439\n",
      "   0.89795917  0.9054054 ]]\n",
      "sigmas0 [[ 0.02759033  0.00194256  0.01775867  0.03035834  0.01180523  0.01866075\n",
      "   0.03527383  0.0640904   0.06314699  0.02359549  0.03104059  0.00453064\n",
      "   0.02002618  0.03906937  0.01024649  0.00331109  0.02084198  0.01096721\n",
      "   0.00379936  0.14292412]\n",
      " [ 0.02759033  0.00194256  0.01775867  0.03035834  0.01180523  0.01866075\n",
      "   0.03527383  0.0640904   0.06314699  0.02359549  0.03104059  0.00453064\n",
      "   0.02002618  0.03906937  0.01024649  0.00331109  0.02084198  0.01096721\n",
      "   0.00379936  0.14292412]\n",
      " [ 0.02759033  0.00194256  0.01775867  0.03035834  0.01180523  0.01866075\n",
      "   0.03527383  0.0640904   0.06314699  0.02359549  0.03104059  0.00453064\n",
      "   0.02002618  0.03906937  0.01024649  0.00331109  0.02084198  0.01096721\n",
      "   0.00379936  0.14292412]]\n",
      "pis [[  9.99894142e-01   1.05832572e-04   5.61148550e-09]\n",
      " [  1.05832572e-04   9.99894142e-01   2.44758424e-13]\n",
      " [  9.14698362e-01   8.53015482e-02   9.88628344e-08]\n",
      " [  9.89067137e-01   1.09324250e-02   3.87533163e-07]\n",
      " [  5.61207969e-09   2.44784336e-13   1.00000000e+00]\n",
      " [  9.78490770e-01   1.69457849e-06   2.15075389e-02]\n",
      " [  2.99909821e-04   2.02898545e-10   9.99700069e-01]\n",
      " [  4.93033379e-01   2.82362021e-06   5.06963789e-01]\n",
      " [  9.98631477e-01   3.66453052e-04   1.00204523e-03]\n",
      " [  6.27624094e-01   3.72375250e-01   6.67772952e-07]]\n",
      "alphas1 [ 0.6001845   0.14689802  0.25291747]\n",
      "mus1 [[ 0.7375015   0.89849502  0.43162623  0.53868067  0.24562259  0.82504368\n",
      "   0.27875385  0.23787977  0.49909115  0.78632879  0.58118457  0.84612942\n",
      "   0.43604335  0.68368757  0.25795871  0.23246682  0.81416917  0.80806863\n",
      "   0.88918734  0.33689871]\n",
      " [ 0.81705618  0.87838125  0.42652243  0.69481128  0.19939217  0.91627109\n",
      "   0.22492574  0.22449549  0.22204961  0.51435041  0.68816864  0.73046076\n",
      "   0.53202665  0.74890035  0.33001721  0.1790826   0.7364136   0.84626824\n",
      "   0.88067102  0.15760487]\n",
      " [ 0.70613837  0.94759405  0.5993982   0.32280993  0.22709256  0.74940407\n",
      "   0.54625088  0.24407369  0.75384396  0.91216558  0.38996053  0.90921587\n",
      "   0.36835015  0.87691975  0.38311374  0.13536462  0.87148178  0.79025233\n",
      "   0.9362039   0.89710373]]\n",
      "diff alpha 0.0\n",
      "diff mu 2.11596e-06\n",
      "sigma (3, 20) \n",
      " [[ 0.0553524   0.00332995  0.0204402   0.04277299  0.01538361  0.03155763\n",
      "   0.05211444  0.07405557  0.08416092  0.02120609  0.0479403   0.0024677\n",
      "   0.01817991  0.08817507  0.0123311   0.00160596  0.02709044  0.01373781\n",
      "   0.0068288   0.20208509]\n",
      " [ 0.00461478  0.00117625  0.00339006  0.00601347  0.00382136  0.00769628\n",
      "   0.01004694  0.09770349  0.05669369  0.00697467  0.00197826  0.00556491\n",
      "   0.00103281  0.00274207  0.00261188  0.00191397  0.04655124  0.00879647\n",
      "   0.00153895  0.00953631]\n",
      " [ 0.00179358  0.001211    0.00421785  0.00993147  0.02497559  0.01292033\n",
      "   0.00475453  0.09712843  0.00768028  0.00319333  0.0039178   0.00317503\n",
      "   0.05324861  0.00415064  0.0073813   0.01228856  0.00203784  0.01752849\n",
      "   0.00351752  0.00137003]]\n"
     ]
    }
   ],
   "source": [
    "## Implement custom GMM\n",
    "#todo check float32/float64 conversion for speedup\n",
    "#todo scale (standardize) data before running\n",
    "#todo kmeans warmup (just in numpy?)\n",
    "#todo mincovar\n",
    "\n",
    "X = tf.Variable(X_trunc, 'X') #tf.placeholder('float', shape=[N, p])\n",
    "\n",
    "alpha0 = tf.ones([K], dtype='float') / K\n",
    "alphas = tf.Variable(alpha0, 'alphas')\n",
    "\n",
    "mu0 = X_trunc[np.random.choice(N, K, replace=False)]\n",
    "mus = tf.Variable(mu0, 'mus')\n",
    "\n",
    "# initialize the off-band with covariances\n",
    "# currently just diagonal\n",
    "# once random init\n",
    "def tf_repeat(x, n):\n",
    "    return tf.pack([x for i in range(n)]) # tf.tile instead. todo\n",
    "sigma0 = tf.nn.moments(X_trunc, [0])[1]\n",
    "sigmas = tf.Variable(tf_repeat(sigma0, K), 'sigmas')\n",
    "\n",
    "# E-step\n",
    "pis = responsibilities(X, mus, sigmas, alphas, K, p)\n",
    "\n",
    "# M-step\n",
    "#http://cslu.ohsu.edu/~bedricks/courses/cs655/pdf/addl_slides/pr813_lecture06.pdfttp://cslu.ohsu.edu/~bedricks/courses/cs655/pdf/addl_slides/pr813_lecture06.pdf\n",
    "pisT = tf.transpose(pis)\n",
    "membership = tf.reduce_sum(pisT, 1)\n",
    "mu1 = tf.matmul(pisT, X) / tf.expand_dims(membership, 1)\n",
    "alpha1 = membership / tf.reduce_sum(membership)\n",
    "\n",
    "# use \n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print('alphas0', sess.run(alphas))\n",
    "print('mus0', sess.run(mus))\n",
    "print('sigmas0', sess.run(sigmas))\n",
    "print('pis', sess.run(pis))\n",
    "print('alphas1', sess.run(alpha1))\n",
    "print('mus1', sess.run(mu1))\n",
    "a, m, s = sess.run(mstep(X, mus, sigmas, alphas, pis))\n",
    "print('diff alpha', sess.run(tf.reduce_sum(tf.abs(a - alpha1))))\n",
    "print('diff mu', sess.run(tf.reduce_sum(tf.abs(m - mu1))))\n",
    "print('sigma', s.shape, '\\n', s)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
